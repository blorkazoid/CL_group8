{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you will be asked to extend the work by Gatti et al by checking whether form-meaning mappings learned on a different yet related language to that considered in the original study still capture the perceived valence of pseudowords. To do this you will be asked to engage with several different resources and adapt the pipeline following the instructions. Along the way, you will be asked to answer a few questions.\n",
        "\n",
        "You need to submit the complete notebook in .ipynb format, with intermediate outputs visible. The notebook should be named as follows:\n",
        "\n",
        "CL2025_groupN_assignment.ipynb\n",
        "\n",
        "where N is the group number. Submissions in the wrong format or with names not adhering to the guidelines will not be evaluated."
      ],
      "metadata": {
        "id": "GcbFlt0uJpNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicate group members' names, student numbers, and contributions below:\n",
        "- 1.\n",
        "- 2.\n",
        "- 3.\n",
        "- 4.\n",
        "- 5."
      ],
      "metadata": {
        "id": "6rkN7t4rqacE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##I suggest that we use \"##\" for comments instead, to distinguish between our and the original comments, but idk if there is any standard for this -Frey"
      ],
      "metadata": {
        "id": "_Z_WEj_70sxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##To do:\n",
        "- get the files into colab without having to rerun the code each time\n",
        "- check if frey found the correct files"
      ],
      "metadata": {
        "id": "SZ9jzt3t0zgS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OQbS5Urfit8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6fb09970-dffd-4ddf-9ac7-01f4c21b4506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'psycho-embeddings'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 199 (delta 105), reused 141 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (199/199), 67.91 KiB | 539.00 KiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n",
            "/content/psycho-embeddings\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# the code has been tested using the psycho-embeddings library to extract representations from LLMs. You can also use other libraries,\n",
        "# as long as you make sure that you are producing the correct output.\n",
        "!git clone https://github.com/MilaNLProc/psycho-embeddings.git\n",
        "%cd psycho-embeddings\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Some pip installs I added -Frey\n",
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejSAG2VY5CwS",
        "outputId": "30c848ae-a312-474c-e7e4-5a13a21fade4",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313506 sha256=f2976455713d6621b9a47572899a71d0a766dd7ae178e3f88c410272b9bfa500\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ofb0L_c0AW0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78493c74-496d-4c98-b64d-6d7ba279c475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
          ]
        }
      ],
      "source": [
        "# the solution to the assignment has been obtained using these packages.\n",
        "# you're free to use other packages though: consider this as an indication, not a prescription.\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fasttext as ft\n",
        "import pickle as pkl\n",
        "import fasttext.util\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "from psycho_embeddings import ContextualizedEmbedder\n",
        "\n",
        "## psycho_embeddings throws a problems about tensorflow versions, but these are the same as the lecture 5 notebook\n",
        "## so I assume they can be ignored"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1** (*10 points available, see breakdown per task below*)\n",
        "\n",
        "You should replicate the main design in the paper *Valence without meaning* by Gatti and colleagues (2024), using estimates collected for Dutch word valence to train linear regression models and apply them to predict the valence of English pseudowords from Gatti and colleagues.\n",
        "\n",
        "In detail, to train your regression models, you should use the dataset by Speed and Brysbaert (2024) containing crowd-sourced valence ratings (use the metadata to identify the relevant columns) collected for approximately 24,000 Dutch words. See the paper *Ratings of valence, arousal, happiness, anger, fear, sadness, disgust, and surprise for 24,000 Dutch words* by Speed and Brysbaert (2024).\n",
        "\n",
        "You should train a letter unigram model and a bigram model. Each model should be trained on Dutch words only.\n",
        "\n",
        "Pay attention to one issue though: pseudowords created for English may be valid words in Dutch: therefore, you should first filter the list of pseudowords against a large store of Dutch words. To do so, use the words in the Dutch prevalence lexicon available in this OSF repository: https://osf.io/9zymw/. Essentially, you need to exclude any pseudoword that happens to be a word for which a prevalence estimate is available, whatever the prevalence is.\n",
        "\n",
        "Each code block indicates how many points are available and how they are attributed."
      ],
      "metadata": {
        "id": "Ra3phcRhKuuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "Freys comments \\\n",
        "IF SOMEONE COULD CHECK IF I HAVE THE CORRECT FILES \\\n",
        "\n",
        "Link to Speed and Brysbaert paper: https://link-springer-com.tilburguniversity.idm.oclc.org/article/10.3758/s13428-023-02239-6 \\\n",
        "Link to Valence data: https://osf.io/9htuv/files/osfstorage (data -> valence) \\\n",
        "  \\\n",
        "Gatti file link: https://osf.io/kv9at/files/osfstorage \\\n",
        "There is several files in the Rdata file, experiment3 files contain a subset of the words of experiment2 files (so I am loading in exp.2).\n",
        "Since we just want the words, which file from the experiment, should not matter. \\\n",
        "pseudowords_Gatti is called comb_2 in the Rdata file\n",
        "\n",
        "\n",
        "\n",
        "-----------"
      ],
      "metadata": {
        "id": "NsWwywDCYJQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hrd4EhHlAcmi"
      },
      "outputs": [],
      "source": [
        "# read in the pseudowords from Gatti and colleagues, as well as the valence ratings for 24,000 Dutch words from Speed and Brysbaert (2024)\n",
        "# show the first 5 lines of each dataset.\n",
        "# 1 point for identifying the correct files and correctly loading their content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## getting the pseudowords from Gatti -Frey\n",
        "\n",
        "## I am using a pandas dataframe (df)\n",
        "## pseudowords_Gatti is called comb_2 in the Rdata file\n",
        "\n",
        "df_Gatti = pd.read_csv(\"/content/pseudowords_Gatti.csv\")\n",
        "del df_Gatti['Unnamed: 0']\n",
        "df_Gatti.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xx374lmMOEY_",
        "outputId": "8c42e3b4-def3-4cc0-a922-e1d9ff0900fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Word    Value1    Value2\n",
              "0  abhert  0.473009  0.406491\n",
              "1  abhict  0.375453  0.472723\n",
              "2  acleat  0.583840  0.496628\n",
              "3  acmure  0.607354  0.597101\n",
              "4   acoed  0.526847  0.551518"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f15ad938-6a84-40f7-8bed-7b577019c52f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Value1</th>\n",
              "      <th>Value2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abhert</td>\n",
              "      <td>0.473009</td>\n",
              "      <td>0.406491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>abhict</td>\n",
              "      <td>0.375453</td>\n",
              "      <td>0.472723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>acleat</td>\n",
              "      <td>0.583840</td>\n",
              "      <td>0.496628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>acmure</td>\n",
              "      <td>0.607354</td>\n",
              "      <td>0.597101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>acoed</td>\n",
              "      <td>0.526847</td>\n",
              "      <td>0.551518</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f15ad938-6a84-40f7-8bed-7b577019c52f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f15ad938-6a84-40f7-8bed-7b577019c52f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f15ad938-6a84-40f7-8bed-7b577019c52f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f55789d8-3dd6-4805-8a4a-a05192debe07\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f55789d8-3dd6-4805-8a4a-a05192debe07')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f55789d8-3dd6-4805-8a4a-a05192debe07 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_Gatti",
              "summary": "{\n  \"name\": \"df_Gatti\",\n  \"rows\": 1500,\n  \"fields\": [\n    {\n      \"column\": \"Word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          \"slearting\",\n          \"vimby\",\n          \"exgise\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Value1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10064524246073675,\n        \"min\": 0.132264294898,\n        \"max\": 0.816203709963,\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          0.475052598434,\n          0.618798244189,\n          0.397224750447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Value2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1004462683399495,\n        \"min\": 0.17396048662,\n        \"max\": 0.856168666588,\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          0.363447360307,\n          0.65478160088,\n          0.407336970175\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the word valence from Speed -Frey\n",
        "\n",
        "## I have uploaded the file as wordvalence_Speed.csv after I have saved \"All_Valence\" as a csv\n",
        "df_Speed = pd.read_csv(\"/content/wordvalence_Speed.csv\")\n",
        "df_Speed.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BgBEZuGeOEDH",
        "outputId": "9a606ec0-f229-49fb-9fde-8a3d3959758c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      List   Participant Word  Valence  Unknown  RemoveParticipant\n",
              "0  Lijst 5   Lijst 5_PP1  aai      5.0        0                  0\n",
              "1  Lijst 5  Lijst 5_PP11  aai      3.0        0                  0\n",
              "2  Lijst 5  Lijst 5_PP12  aai      3.0        0                  0\n",
              "3  Lijst 5   Lijst 5_PP2  aai      3.0        0                  0\n",
              "4  Lijst 5   Lijst 5_PP3  aai      4.0        0                  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48f720b9-2008-45ec-a37f-3e0a9cf6c578\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>List</th>\n",
              "      <th>Participant</th>\n",
              "      <th>Word</th>\n",
              "      <th>Valence</th>\n",
              "      <th>Unknown</th>\n",
              "      <th>RemoveParticipant</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lijst 5</td>\n",
              "      <td>Lijst 5_PP1</td>\n",
              "      <td>aai</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lijst 5</td>\n",
              "      <td>Lijst 5_PP11</td>\n",
              "      <td>aai</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lijst 5</td>\n",
              "      <td>Lijst 5_PP12</td>\n",
              "      <td>aai</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lijst 5</td>\n",
              "      <td>Lijst 5_PP2</td>\n",
              "      <td>aai</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lijst 5</td>\n",
              "      <td>Lijst 5_PP3</td>\n",
              "      <td>aai</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48f720b9-2008-45ec-a37f-3e0a9cf6c578')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48f720b9-2008-45ec-a37f-3e0a9cf6c578 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48f720b9-2008-45ec-a37f-3e0a9cf6c578');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-322ca3b6-f95a-452d-b8c2-e865cc55fe13\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-322ca3b6-f95a-452d-b8c2-e865cc55fe13')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-322ca3b6-f95a-452d-b8c2-e865cc55fe13 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_Speed"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out pseudowords that happen to be valid Dutch words (mind case folding!)\n",
        "# show the set of pseudowords filtered out.\n",
        "# 1 point for applying the correct filtering"
      ],
      "metadata": {
        "id": "As4XV-LQPbyH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Combining the file for the prevalence of Dutch words and Belgian words -Frey\n",
        "df_prevalence_Netherlands = pd.read_csv(\"/content/prevalence_netherlands.csv\" , sep = \"\\t\")\n",
        "df_prevalence_Belgium = pd.read_csv(\"/content/prevalence_belgium.csv\", sep= \"\\t\")\n",
        "\n",
        "df_prevalence_combined = pd.concat([df_prevalence_Belgium, df_prevalence_Netherlands], join= \"outer\") #outer: takes union\n",
        "\n",
        "## Checking if they all have a prevalence value (they do)\n",
        "df_prevalence_combined_filtered = df_prevalence_combined[df_prevalence_combined[\"prevalence\"] != None]\n",
        "\n",
        "## Saving only the column with words as a Series\n",
        "df_Dutch_words = df_prevalence_combined_filtered.word\n",
        "df_Dutch_words = df_Dutch_words.drop_duplicates()\n",
        "\n",
        "## Saving only the column with pseudowords from pseudowords_Gatti as a Series\n",
        "df_pseudowords = df_Gatti.Word\n",
        "\n",
        "## Apply filtering here\n",
        "print(df_pseudowords[df_pseudowords.isin(df_Dutch_words)])\n",
        "df_pseudowords = df_pseudowords[~df_pseudowords.isin(df_Dutch_words)]"
      ],
      "metadata": {
        "id": "90j_UjCpZbWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d3e57d-cce5-499a-db1e-c4b348ec2d21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900    pimpen\n",
            "Name: Word, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode Dutch words and pseudowords from Gatti et al as uni- and bi-gram vectors\n",
        "# show the uni-gram and bi-gram encoding of the pseudoword ampgrair\n",
        "# 2 points for correctly encoding the target strings as uni- and bi-gram vectors"
      ],
      "metadata": {
        "id": "zvza9pb6MFgC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## this is simplified code from class one, we will have to see if that is plagiarism and has to be rewritten\n",
        "\n",
        "def ngram_featurizer(s, n):\n",
        "\n",
        "    string_boundary = [\"letter\"]*(n-1)                        # necessary to encode features such as 'this string begins/ends with this specific symbol'\n",
        "    s = string_boundary + list(s) + string_boundary\n",
        "\n",
        "    return [tuple(s[i:i+n]) for i in range(len(s)-n+1)]       # this is where the n-gram featurization actually happens.\n"
      ],
      "metadata": {
        "id": "dGbSIqOcDww5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## this is simplified code from class one, we will have to see if that is plagiarism and has to be rewritten\n",
        "\n",
        "# This function encodes all tweets as frequency counts over n-grams\n",
        "def encode_corpus(corpus, n, mapping=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Takes in\n",
        "      - a list of strings,\n",
        "      - an integer indicating the n-grams size,\n",
        "      - a dictionary mapping ngrams to numerical indices. If no dictionary is\n",
        "          passed, one is created inside the function.\n",
        "    The function outputs a 2d NumPy array with as many rows as there are strings in\n",
        "    the input list, and the mapping from ngrams to indices, representing the columns\n",
        "    of the NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    if not mapping:\n",
        "        all_ngrams = set()\n",
        "        for instance in corpus:\n",
        "            # get a comprehensive set of all n-grams in the corpus\n",
        "            all_ngrams = all_ngrams.union(\n",
        "                set(ngram_featurizer(instance, n))\n",
        "                )\n",
        "\n",
        "        # map each n-gram to an integer which will index the feature matrix\n",
        "        mapping = {ngram: i for i, ngram in enumerate(sorted(all_ngrams))}\n",
        "\n",
        "    # create a feature matrix of the appropriate dimensionality\n",
        "    X = np.zeros((len(corpus), len(mapping)))\n",
        "    for i, instance in enumerate(corpus):\n",
        "        for ngram in ngram_featurizer(instance, n):\n",
        "            try:\n",
        "                # access the right column given the n-gram being processed\n",
        "                X[i, mapping[ngram]] += 1\n",
        "            except KeyError:\n",
        "                # if the current n-gram is new, skip it\n",
        "                pass\n",
        "\n",
        "    return X, mapping\n",
        "\n",
        "feature_matrix_unigram, mapping_unigram = encode_corpus(list(df_pseudowords), 1)\n",
        "feature_matrix_bigram, mapping_bigram = encode_corpus(list(df_pseudowords), 2)"
      ],
      "metadata": {
        "id": "3LQUMVSiungL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## WORK IN PROGRESS vv"
      ],
      "metadata": {
        "id": "OmY1BhuP0T5Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## trying to map all the items to their vector here (unsuccesfully)\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "encode_corpus_unigram = partial(encode_corpus, n=1, mapping=mapping_unigram)\n",
        "# list_pseudowords_vectors = map(encode_corpus_unigram, df_pseudowords)\n",
        "\n",
        "# df_pseudowords.map(encode_corpus_unigram)\n",
        "\n",
        "print(df_pseudowords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJYkFr5qxbU8",
        "outputId": "6d1a3a38-741c-4a71-c093-aa40a1735f37"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       abhert\n",
            "1       abhict\n",
            "2       acleat\n",
            "3       acmure\n",
            "4        acoed\n",
            "         ...  \n",
            "1495     zauze\n",
            "1496     zerow\n",
            "1497      zilk\n",
            "1498    zohels\n",
            "1499    zokils\n",
            "Name: Word, Length: 1499, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## testing that ampgrair can be encoded\n",
        "\n",
        "# feature_matrix_ampgrair_unigram, _ = encode_corpus(['ampgrair'], 1, mapping=mapping_unigram)\n",
        "# print(feature_matrix_ampgrair_unigram)\n",
        "\n",
        "# print(\"\")\n",
        "# feature_matrix_ampgrair_bigram, _ = encode_corpus(['ampgrair'], 2, mapping=mapping_bigram)\n",
        "# print(feature_matrix_ampgrair_bigram)"
      ],
      "metadata": {
        "id": "GwrMU0H0wlm9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use word valence estimates from Speed and Brysbaert (2024) to train\n",
        "# - a uni-gram model\n",
        "# - a bi-gram model\n",
        "# 2 points for correctly trained models"
      ],
      "metadata": {
        "id": "-w9-ySEuPp1Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply trained models to predict the valence of pseudowords from Gatti et al (2024).\n",
        "# Then apply the same models back onto the training set to see how well they predict the valence of words in Speed and Brysbaert (2024).\n",
        "# 2 points for correctly applied models"
      ],
      "metadata": {
        "id": "6RG3aeRSPtLX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the Spearman correlation coefficients between true valence and predicted valence under both uni- and bi-gram models for\n",
        "# - words from Speed and Brysbaert (2024)\n",
        "# - pseudowords from Gatti and colleagues (2024)\n",
        "# show both correlation coefficients.\n",
        "# 2 points for the correct Spearman correlation coefficients (rounded to the third decimal place)"
      ],
      "metadata": {
        "id": "9aDwbajtPxze"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2** (*8 points available, see breakdown below*)\n",
        "\n",
        "Again following Gatti and colleagues, you should encode the target strings (pseudowords and Dutch words from Speed and Brysbaert) as fastText embeddings, train a multiple regression model on Dutch words and apply it to the pseudowords in Gatti et al. You should finally report the Spearman correlation coefficient between observed and predicted valence for both words and pseudowords.\n",
        "\n",
        "You should use the pre-trained fastText model for Dutch, available at this page: https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n",
        "Finally, you should answer two questions about the fastText model (see below)."
      ],
      "metadata": {
        "id": "p0P1rNg5QoDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pDeWgWUNAckd"
      },
      "outputs": [],
      "source": [
        "# load the fastText model\n",
        "# 1 point for correctly loading the appropriate fastText model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the dimensionality of the pre-trained Dutch fastText embeddings? (*1 point for the correct answer*)"
      ],
      "metadata": {
        "id": "rUD0VUJeRhr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What minimum and maximum n-gram size was specified for training this fastText model? (*1 point for the correct answer*)"
      ],
      "metadata": {
        "id": "pgej4BPNRoUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode Dutch words and pseudowords as fastText embeddings\n",
        "# show the first 20 values of the embedding of the word 'speelplaats' and of the pseudoword 'danchunk'\n",
        "# 2 points for correctly encoding words and pseudowords with fastText"
      ],
      "metadata": {
        "id": "aW-XEksGR28U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train regression model on word valence\n",
        "# 1 point for correctly training the regression model"
      ],
      "metadata": {
        "id": "3ePBth7cSAJU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the trained model to predict the valence of pseudowords from Gatti et al (2024).\n",
        "# Then apply the same model back onto the training set to see how well it predicts the valence of words in Speed and Brysbaert (2024).\n",
        "# 1 point for correctly applied model"
      ],
      "metadata": {
        "id": "REJnnM2mSHHK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the Spearman correlation coefficients between true valence and predicted valence for\n",
        "# - words from Speed and Brysbaert (2024)\n",
        "# - pseudowords from Gatti and colleagues (2024)\n",
        "# show the correlation coefficient.\n",
        "# 1 point for the correct Spearman correlation coefficients (rounded to the third decimal place)"
      ],
      "metadata": {
        "id": "JIyyTyfHSKh5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3** (*6 points available, see breakdown below*)\n",
        "\n",
        "Now you are asked to extend the work by Gatti et al by also considering the representations learned by a transformer-based models, in detail *RobBERT v2* (https://huggingface.co/pdelobelle/robbert-v2-dutch-base). You should follow the same pipeline as for the previous models, encoding both Dutch words from Speed and Brysbaert (2024) and the pseudowords from Gatti et al using the embedding of each string at layer 0, before positional information is factored in. If a string consists of multiple tokens, average the embeddings of all tokens to produce the embedding of the whole string. Then train a multiple regression model on the valence of Dutch words, apply it to the pseudowords, and compute the Spearman correlation between observed and predicted ratings.\n",
        "\n",
        "Use the HuggingFace model card for RobBERT v2 to check how to access it.\n",
        "\n",
        "I recommend saving the embeddings to file once you have generated them and you know they are correct: embedding thousands of strings takes some time, and you don't want to have to do it again. For the same reason, develop your code by considering only a small fractions of the words and pseudowords, in order to quickly see if something is wrong. Only when you are positive it works, embed all strings."
      ],
      "metadata": {
        "id": "wnqKJ5XOSTbM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "Ppi-Zcp6i9Rn"
      },
      "outputs": [],
      "source": [
        "# load and instantiate the right model\n",
        "# 1 point for loading the right model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the words and pseudowords using RobBERT v2. I've used the free GPU runtime on COLAB to speed things up,\n",
        "# but in this case you need to batch the words and pseudowords. You can use the function below to create batches\n",
        "# but you will have to pay attention at how you store embeddings.\n",
        "# show the first 20 values of the embedding of the word 'miauwen' and of the pseudoword 'lixthless'\n",
        "# 2 points for correctly encoding words and pseudowords\n",
        "\n",
        "def chunks(lst, n):\n",
        "\n",
        "    \"\"\"Chunks a list into equal chunks containing n elements. Returns a list of lists.\"\"\"\n",
        "\n",
        "    chunked = []\n",
        "    for i in range(0, len(lst), n):\n",
        "        chunked.append(lst[i:i + n])\n",
        "    return chunked\n"
      ],
      "metadata": {
        "id": "nGBaQgZqZzhw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train regression model on word valence estimates from Speed and Brysbaert (2024)\n",
        "# 1 point for correctly training the regression model"
      ],
      "metadata": {
        "id": "BFq3hHCDUPjL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the trained model to predict the valence of pseudowords from Gatti et al (2024).\n",
        "# Then apply the same model back onto the training set to see how well it predicts the valence of words in Speed and Brysbaert (2024).\n",
        "# 1 point for correctly applied model"
      ],
      "metadata": {
        "id": "evaU9NAxUSoW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the Spearman correlation coefficients between true valence and predicted valence for\n",
        "# - words from Speed and Brysbaert (2024)\n",
        "# - pseudowords from Gatti and colleagues (2024)\n",
        "# show the correlation coefficient\n",
        "# 1 point for the correct Spearman correlation coefficients (rounded to the third decimal place)"
      ],
      "metadata": {
        "id": "JVcuHS02UUPd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfWX1QTfB172"
      },
      "source": [
        "**Task 4** (*16 points available, 4 for each question*)\n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "**4a.** Describe the performance of each featurization, comparing\n",
        "- the performance of a same model between the training and test set\n",
        "- the performance of different models on the training set\n",
        "- the performance of different models on the test set\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EONmoGe8CAyI"
      },
      "source": [
        "*type your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpkgvOqeB6jH"
      },
      "source": [
        "**4b.** Compare the correlations you found when training uni-gram, bi-gram, and fastText models on Dutch words and the correlations of similar models trained on English data as reported by Gatti and colleagues; summarize the most important similarities and differences.\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv7P2zvnCBiX"
      },
      "source": [
        "*type your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ2SxrYHy3Hx"
      },
      "source": [
        "**4c.** Do you think the performance of the fastText featurization would change if you were to use different n-grams? Would you make them smaller or larger? Justify your answer.\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*type your answer here*"
      ],
      "metadata": {
        "id": "-M-lvw2qVjNH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_-zN3Vp2OBD"
      },
      "source": [
        "**4d.** Do you think that training the same models on uni-grams, bi-grams, fastText and transformer-based embeddings but using valence ratings for Finnish (a language which uses the same alphabet as English but is not a IndoEuropean language) words would yield a similar pattern of results? Justify your answer.\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*type your answer here*"
      ],
      "metadata": {
        "id": "20T-4kCdVppE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5** (*3 points available*)\n",
        "\n",
        "Compute the average Levenshtein Distance (aLD) between each pseudoword and the 20 words at the smallest edit distance from it. Consider the set of words you used to filter out pseudowords that happen to be valid Dutch words (the file is available in this OSF repository: https://osf.io/9zymw/) to retrieve the 20 words at the smallest edit distance."
      ],
      "metadata": {
        "id": "w4ILTPziXptK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OGks7N-JCjFu"
      },
      "outputs": [],
      "source": [
        "# compute the average Levenshtein distance from each pseudoword to the words used to filter out pseudowords.\n",
        "# Show the aLD estimate for the pseudowords 'nedukes', 'pewbin', and 'vibcines'\n",
        "# 3 points for correctly computing aLD for pseudowords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6** (*3 points available*)\n",
        "\n",
        "For each pseudoword, record the number of tokens in which RobBERT v2 encodes it."
      ],
      "metadata": {
        "id": "zBdwMhHsYY0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FDOechQfmvqE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# record the number of tokens in which RobBERT divides each pseudoword\n",
        "# show the number of tokens for the pseudowords 'yuxwas', 'skibfy', and 'errords'\n",
        "# 3 points for correctly mapping pseudowords to number of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 7** (*5 points available, see breakdown below*)\n",
        "\n",
        "Compute the residuals of the predicted valence under the four regressors trained and applied in tasks 2 to 4. Then, correlate the residuals from all four models with aLD. Finally, correlate the residuals from the RobBERT v2 model with the number of tokens in which each pseudoword is split. Use the Pearson's correlation coefficient."
      ],
      "metadata": {
        "id": "H9LxipdMYqXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dc2p7UXSCi-q"
      },
      "outputs": [],
      "source": [
        "# compute the residuals from all four regression models fitted before\n",
        "# 1 point available for correctly computing residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KkqJLI17C0Ml"
      },
      "outputs": [],
      "source": [
        "# compute the Pearson's correlation between residuals and average LD for all models,\n",
        "# as well as the correlation between RobBERT v2 residuals and the number of tokens in which each pseudoword\n",
        "#    is encoded by the RobBERT v2 model.\n",
        "# show all correlation coefficients\n",
        "# 4 points for the correct correlation coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6owroLfAC4vf"
      },
      "source": [
        "**Task 8** What is the relation between the errors each model made and aLD? what about the number of tokens (limited to the RobBERT v2 model)?\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*testo in corsivo*"
      ],
      "metadata": {
        "id": "LvaOAjqxuHgm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}